#!/usr/bin/env python
# -*- coding: utf-8 -*-

####################################################################################
# Integron_Finder - Integron Finder aims at detecting integrons in DNA sequences   #
# by finding particular features of the integron:                                  #
#   - the attC sites                                                               #
#   - the integrase                                                                #
#   - and when possible attI site and promoters.                                   #
#                                                                                  #
# Authors: Jean Cury, Bertrand Neron, Eduardo PC Rocha                             #
# Copyright Â© 2015 - 2016  Institut Pasteur, Paris.                                #
# See the COPYRIGHT file for details                                               #
#                                                                                  #
# integron_finder is free software: you can redistribute it and/or modify          #
# it under the terms of the GNU General Public License as published by             #
# the Free Software Foundation, either version 3 of the License, or                #
# (at your option) any later version.                                              #
#                                                                                  #
# integron_finder is distributed in the hope that it will be useful,               #
# but WITHOUT ANY WARRANTY; without even the implied warranty of                   #
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the                    #
# GNU General Public License for more details.                                     #
#                                                                                  #
# You should have received a copy of the GNU General Public License                #
# along with this program (COPYING file).                                          #
# If not, see <http://www.gnu.org/licenses/>.                                      #
####################################################################################


"""
integron_finder is a program that looks for integron in DNA sequences.
"""

__version__ = '$VERSION'

import glob
import numpy as np
import pandas as pd
import platform

if not __version__.endswith('VERSION'):
    # display warning only for non installed integron_finder
    from Bio import BiopythonExperimentalWarning
    import warnings
    warnings.simplefilter('ignore', FutureWarning)
    warnings.simplefilter('ignore', BiopythonExperimentalWarning)

from Bio import SeqIO
from Bio import SearchIO
from Bio import motifs
from Bio import Seq
from Bio import SeqFeature
from subprocess import call
import os
import sys
import argparse
from matplotlib import use as m_use
m_use("Agg")
import matplotlib.pyplot as plt
import distutils.spawn



class IntegronError(Exception):
    pass


class Integron(object):
    """Integron object represents an object composed of an integrase, attC sites and gene cassettes.
    Each element is characterized by their coordinates in the replicon, the strand (+ or -),
    the ID of the gene (except attC).
    The object Integron is also characterized by the ID of the replicon."""

    def __init__(self, ID_replicon):
        self.ID_replicon = ID_replicon
        self._columns = ["pos_beg", "pos_end", "strand", "evalue", "type_elt", "model", "distance_2attC", "annotation"]
        self._dtype = {"pos_beg": "int",
                       "pos_end": "int",
                       "strand": "int",
                       "evalue": "float",
                       "type_elt": "str",
                       "model": "str",
                       "distance_2attC": "float",
                       "annotation": "str"}
        self.integrase = pd.DataFrame(columns=self._columns)
        self.integrase = self.integrase.astype(dtype=self._dtype)

        self.attC = pd.DataFrame(columns=self._columns)
        self.attC = self.attC.astype(dtype=self._dtype)

        self.promoter = pd.DataFrame(columns=self._columns)
        self.promoter = self.promoter.astype(dtype=self._dtype)

        self.attI = pd.DataFrame(columns=self._columns)
        self.attI = self.attI.astype(dtype=self._dtype)

        self.proteins = pd.DataFrame(columns=self._columns)
        self.proteins = self.proteins.astype(dtype=self._dtype)

    @property
    def dtype(self):
        return {k: v for k, v in self._dtype.items()}

    def add_integrase(self, pos_beg_int, pos_end_int, id_int, strand_int, evalue, model):
        """Function which adds integrases to the integron. Should be called once"""
        if not self.integrase.empty:
            raise RuntimeError("add_integrase should be called once.")
        tmp_df = pd.DataFrame(columns=self._columns)
        tmp_df = tmp_df.astype(dtype=self._dtype)
        tmp_df["pos_beg"] = [pos_beg_int]
        tmp_df["pos_end"] = [pos_end_int]
        tmp_df["strand"] = [strand_int]
        tmp_df["evalue"] = [evalue]
        tmp_df["type_elt"] = "protein"
        tmp_df["annotation"] = "intI"
        tmp_df["model"] = [model]
        tmp_df.index = [id_int]
        tmp_df["distance_2attC"] = [np.nan]
        self.integrase = self.integrase.append(tmp_df)

    def add_attC(self, pos_beg_attC, pos_end_attC, strand, evalue, model):
        """ Function which adds attC site to the Integron object. """
        tmp_df = pd.DataFrame(columns=self._columns)
        tmp_df = tmp_df.astype(dtype=self._dtype)
        tmp_df["pos_beg"] = [pos_beg_attC]
        tmp_df["pos_end"] = [pos_end_attC]
        tmp_df["strand"] = [strand]
        tmp_df["evalue"] = [evalue]
        tmp_df["type_elt"] = "attC"
        tmp_df["annotation"] = "attC"
        tmp_df["model"] = [model]
        self.attC = self.attC.append(tmp_df, ignore_index=True)
        attC_len = len(self.attC)
        if attC_len < 2:
            self.sizes_cassettes = [np.nan]
        else:
            self.sizes_cassettes.append((self.attC.iloc[attC_len - 1].pos_beg -
                                     self.attC.iloc[attC_len - 2].pos_end) % SIZE_REPLICON)
        self.attC["distance_2attC"] = self.sizes_cassettes

        #self.attC.sort_values(["pos_beg"], inplace = True)
        self.attC.index = ["attc_%03i" % int(j + 1) for j in self.attC.index]

    def type(self):
        """
        Tells you whether the integrons is :
        - complete : Have one integrase and at least one attC
        - CALIN : Have at least one attC
        - In0 : Just an integrase intI
        """
        if len(self.attC) >= 1 and len(self.integrase) == 1:
            return "complete"
        elif len(self.attC) == 0 and len(self.integrase) == 1:
            return "In0"
        elif len(self.attC) >= 1 and len(self.integrase) == 0:
            return "CALIN"

    def add_promoter(self):
        """
        Function that looks for known promoters if they exists within your integrons element.
        It takes 1s for about 13kb.
        """
        dist_prom = 500  # pb distance from edge of the element for which we seek promoter

        ######## Promoter of integrase #########

        if self.has_integrase():
            ## PintI1
            p_intI1 = motifs.create([Seq.Seq("TTGCTGCTTGGATGCCCGAGGCATAGACTGTACA")])
            p_intI1.name = "P_intI1"

            ## PintI2
            ## Not known

            ## PintI3
            ## Not known

            motifs_Pint = [p_intI1]

            seq_p_int = SEQUENCE.seq[int(self.integrase.pos_beg.min()) - dist_prom : int(self.integrase.pos_end.max()) + dist_prom]

            for m in motifs_Pint:
                if self.integrase.strand.values[0] == 1:
                    generator_motifs = m.instances.search(seq_p_int[:dist_prom])
                    for pos, s in generator_motifs:
                        tmp_df = pd.DataFrame(columns=self._columns)
                        tmp_df = tmp_df.astype(dtype=self._dtype)
                        tmp_df["pos_beg"] = [self.integrase.pos_beg.values[0] - dist_prom + pos]
                        tmp_df["pos_end"] = [self.integrase.pos_beg.values[0] - dist_prom + pos + len(s)]
                        tmp_df["strand"] = [self.integrase.strand.values[0]]
                        tmp_df["evalue"] = [np.nan]
                        tmp_df["type_elt"] = "Promoter"
                        tmp_df["annotation"] = "Pint_%s" %(m.name[-1])
                        tmp_df["model"] = "NA"
                        tmp_df.index = [m.name]
                        tmp_df["distance_2attC"] = [np.nan]
                        self.promoter = self.promoter.append(tmp_df)
                else:
                    generator_motifs = m.instances.reverse_complement().search(seq_p_int[-dist_prom:])
                    for pos, s in generator_motifs:
                        tmp_df = pd.DataFrame(columns=self._columns)
                        tmp_df = tmp_df.astype(dtype=self._dtype)
                        tmp_df["pos_beg"] = [self.integrase.pos_end.max() + pos]
                        tmp_df["pos_end"] = [self.integrase.pos_end.max() + pos + len(s)]
                        tmp_df["strand"] = [self.integrase.strand.values[0]]
                        tmp_df["evalue"] = [np.nan]
                        tmp_df["type_elt"] = "Promoter"
                        tmp_df["annotation"] = "Pint_%s" % (m.name[-1])
                        tmp_df["model"] = "NA"
                        tmp_df.index = [m.name]
                        tmp_df["distance_2attC"] = [np.nan]
                        self.promoter = self.promoter.append(tmp_df)

        ######## Promoter of K7 #########

        ## Pc-int1
        motifs_Pc = []

        pc = SeqIO.parse(os.path.join(MODEL_DIR, "variants_Pc_intI1.fst"), "fasta")
        pseq = [i for i in pc]
        d = {len(i): [] for i in pseq}
        _ = [d[len(i)].append(i.seq.upper()) for i in pseq]
        for k, i in d.iteritems():
            motifs_Pc.append(motifs.create(i))
            motifs_Pc[-1].name = "Pc_int1"

        ## Pc-int2
        ## Not known

        ## Pc-int3

        pc_intI3 = motifs.create([Seq.Seq("TAGACATAAGCTTTCTCGGTCTGTAGGCTGTAATG"),
                                  Seq.Seq("TAGACATAAGCTTTCTCGGTCTGTAGGATGTAATG")])
        #                                                             *
        pc_intI3.name = "Pc_int3"

        motifs_Pc.append(pc_intI3)

        if self.type() == "complete":

            if ((self.attC.pos_beg.values[0] - self.integrase.pos_end.values[0]) % SIZE_REPLICON >
                (self.integrase.pos_beg.values[0] - self.attC.pos_end.values[-1]) % SIZE_REPLICON):
                # if integrase after attcs (on the right)
                left = int(self.attC.pos_end.values[-1])
                right = int(self.integrase.pos_beg.values[0])
            else:
                left = int(self.integrase.pos_end.values[-1])
                right = int(self.attC.pos_beg.values[0])

            strand_array = self.attC.strand.unique()[0]

        elif self.type() == "In0":
            left = int(self.integrase.pos_beg.values[0])
            right = int(self.integrase.pos_end.values[-1])
            strand_array = "both"

        elif self.type() == "CALIN":
            left = int(self.attC.pos_beg.values[0])
            right = int(self.attC.pos_end.values[-1])
            strand_array = self.attC.strand.unique()[0]

        if left < right:
            seq_Pc = SEQUENCE.seq[left - dist_prom : right + dist_prom]
        else:
            seq_Pc1 = SEQUENCE.seq[left - dist_prom : SIZE_REPLICON]
            seq_Pc2 = SEQUENCE.seq[:right + dist_prom]
            seq_Pc = seq_Pc1 + seq_Pc2

        for m in motifs_Pc:
            if strand_array == 1:
                mot = [m]
            elif strand_array == "both":
                mot = [m.reverse_complement(), m]
            else:
                mot = [m.reverse_complement()]

            for sa, mo in enumerate(mot):
                for pos, s in mo.instances.search(seq_Pc):
                    tmp_df = pd.DataFrame(columns=self._columns)
                    tmp_df = tmp_df.astype(dtype=self._dtype)
                    tmp_df["pos_beg"] = [(left - dist_prom + pos) % SIZE_REPLICON]
                    tmp_df["pos_end"] = [(left - dist_prom + pos + len(s)) % SIZE_REPLICON]
                    tmp_df["strand"] = [strand_array] if strand_array != "both" else [sa * 2 - 1]
                    tmp_df["evalue"] = [np.nan]
                    tmp_df["type_elt"] = "Promoter"
                    tmp_df["annotation"] = "Pc_%s" % (m.name[-1])
                    tmp_df["model"] = "NA"
                    tmp_df.index = [m.name]
                    tmp_df["distance_2attC"] = [np.nan]
                    self.promoter = self.promoter.append(tmp_df)


    def add_attI(self):
        dist_atti = 500

        ## attI1
        instances_attI1 = [Seq.Seq('TGATGTTATGGAGCAGCAACGATGTTACGCAGCAGGGCAGTCGCCCTAAAACAAAGTT')]
        attI1 = motifs.create(instances_attI1)
        attI1.name = "attI1"

        ## attI2
        instances_attI2 = [Seq.Seq('TTAATTAACGGTAAGCATCAGCGGGTGACAAAACGAGCATGCTTACTAATAAAATGTT')]
        attI2 = motifs.create(instances_attI2)
        attI2.name = "attI2"

        ## attI3
        instances_attI3 = [Seq.Seq('CTTTGTTTAACGACCACGGTTGTGGGTATCCGGTGTTTGGTCAGATAAACCACAAGTT')]
        attI3 = motifs.create(instances_attI3)
        attI3.name = "attI3"

        motif_attI = [attI1, attI2, attI3]

        if self.type() == "complete":
            if ((self.attC.pos_beg.values[0] - self.integrase.pos_end.values[0]) % SIZE_REPLICON >
                (self.integrase.pos_beg.values[0] - self.attC.pos_end.values[-1]) % SIZE_REPLICON):
                # if integrase after attcs (on the right)

                left = int(self.attC.pos_end.values[-1])
                right = int(self.integrase.pos_beg.values[0])
            else:
                left = int(self.integrase.pos_end.values[-1])
                right = int(self.attC.pos_beg.values[0])
            strand_array = self.attC.strand.unique()[0]

        elif self.type() == "In0":
            left = int(self.integrase.pos_beg)
            right = int(self.integrase.pos_end)
            strand_array = "both"

        elif self.type() == "CALIN":
            left = int(self.attC.pos_beg.values[0])
            right = int(self.attC.pos_end.values[-1])
            strand_array = self.attC.strand.unique()[0]

        if left < right:
            seq_attI = SEQUENCE.seq[left - dist_atti : right + dist_atti]
        else:
            seq_attI1 = SEQUENCE.seq[left - dist_atti : SIZE_REPLICON]
            seq_attI2 = SEQUENCE.seq[:right + dist_atti]
            seq_attI = seq_attI1 + seq_attI2

        for m in motif_attI:

            if strand_array == 1:
                mot = [m]
            elif strand_array == "both":
                mot = [m.reverse_complement(), m]
            else:
                mot = [m.reverse_complement()]

            for sa, mo in enumerate(mot):
                for pos, s in mo.instances.search(seq_attI):
                    tmp_df = pd.DataFrame(columns=self._columns)
                    tmp_df = tmp_df.astype(dtype=self._dtype)
                    tmp_df["pos_beg"] = [(left - dist_atti + pos) % SIZE_REPLICON]
                    tmp_df["pos_end"] = [(left - dist_atti + pos + len(s)) % SIZE_REPLICON]
                    tmp_df["strand"] = [strand_array] if strand_array != "both" else [sa * 2 - 1]
                    tmp_df["evalue"] = [np.nan]
                    tmp_df["type_elt"] = "attI"
                    tmp_df["annotation"] = "attI_%s" % (m.name[-1])
                    tmp_df["model"] = "NA"
                    tmp_df.index = [m.name]
                    tmp_df["distance_2attC"] = [np.nan]
                    self.attI = self.attI.append(tmp_df)


    def add_proteins(self):
        debut = self.attC.pos_beg.values[0]
        fin = self.attC.pos_end.values[-1]

        if self.has_integrase():
            if ((debut - self.integrase.pos_end.values[0]) % SIZE_REPLICON >
                (self.integrase.pos_beg.values[0] - fin) % SIZE_REPLICON):
                # integrase on the right of attC cluster.
                fin = self.integrase.pos_beg.min()
                debut -= 200
            else:
                debut = self.integrase.pos_end.max()
                fin += 200
        else:
            # To allow the first protein after last attC to aggregate.
            debut -= 200
            fin += 200

        for i in SeqIO.parse(PROT_file, "fasta"):
            if not args.gembase:
                desc = [j.strip() for j in i.description.split("#")][:-1]
                start = int(desc[1])
                end = int(desc[2])

            else:
                desc = [j for j in i.description.split(" ")]
                desc = desc[:2] + desc[4:6]
                desc[1] = 1 if desc[1] == "D" else -1
                start = int(desc[2])
                end = int(desc[3])

            s_int = (fin - debut) % SIZE_REPLICON

            if ((fin - end) % SIZE_REPLICON < s_int) or ((start - debut) % SIZE_REPLICON < s_int):
                # We keep proteins (<--->) if start (<) and end (>) follows that scheme:
                #
                # ok:            <--->         <--->
                # ok:  <--->                                    <--->
                #          ^ 200pb v                    v 200pb ^
                #                  |------integron------|
                #                debut                 fin

                prot_annot = "protein"
                prot_evalue = np.nan
                prot_model = "NA"

                if args.gembase:
                    self.proteins.loc[desc[0]] = desc[2:] + [desc[1]] + [prot_evalue, "protein",
                                                                         prot_model, np.nan, prot_annot]
                else:
                    self.proteins.loc[desc[0]] = desc[1:] + [prot_evalue, "protein",
                                                             prot_model, np.nan, prot_annot]
            intcols = ["pos_beg", "pos_end", "strand"]
            floatcols = ["evalue", "distance_2attC"]
            self.proteins[intcols] = self.proteins[intcols].astype(int)
            self.proteins[floatcols] = self.proteins[floatcols].astype(float)


    def describe(self):
        """ Method describing the integron object """

        full = pd.concat([self.integrase, self.attC, self.promoter, self.attI, self.proteins])
        full["pos_beg"] = full["pos_beg"].astype(int)
        full["pos_end"] = full["pos_end"].astype(int)
        full["strand"] = full["strand"].astype(int)
        full["distance_2attC"] = full["distance_2attC"].astype(float)
        full = full.reset_index()
        full.columns = ["element"] + list(full.columns[1:])
        full["type"] = self.type()
        full["ID_replicon"] = self.ID_replicon
        full["ID_integron"] = id(self)  # uniq identifier of a given Integron
        full["default"] = "Yes" if not (args.eagle_eyes or args.local_max) else "No"
        full.drop_duplicates(subset=["element"], inplace=True)
        return full


    def draw_integron(self, file=0):
        """
        Represent the different element of the integrons
        """
        full = self.describe()
        full["evalue"] = full["evalue"].astype("float")
        h = [i + (0.5*i) if j == "Promoter" else i for i, j in zip(full.strand, full.type_elt)]
        fig, ax = plt.subplots(1, 1, figsize=(16, 9))
        alpha = [i if i < 1 else 1 for i in (
                 (np.log10(full.evalue) - np.ones(len(full)) * -1) /
                 (np.ones(len(full)) * -10 - np.ones(len(full)) * -1)
                 * (1 - 0.2) + 0.2).fillna(1).tolist()]
                 # normalize alpha value with 0.2 as min value

        colors = ["#749FCD" if i == "attC" else
                  "#DD654B" if i == "intI" else
                  "#6BC865" if (i[-2:] == "_1" and j == "Promoter") else
                  "#D06CC0" if (i[-2:] == "_2" and j == "Promoter") else
                  "#C3B639" if (i[-2:] == "_3" and j == "Promoter") else
                  "#e8950e" if i != "protein" else
                  "#d3d3d3" for (i, j) in zip(full.annotation,
                                             full.type_elt)]

        colors_alpha = [j+[i] for j, i in zip([[ord(c)/255. for c in i[1:].decode("hex")] for i in colors],
                                              alpha)]


        #ec = ["red" if i =="attC" else
        #      "white" for i in full.type_elt]
        z_order = [100 if i == "attC" else
                   1 for i in full.type_elt]

        ax.barh(np.zeros(len(full)), full.pos_end-full.pos_beg,
                height=h, left=full.pos_beg,
                color=colors_alpha, zorder=z_order, ec=None)  # edgecolor=ec,
        xlims = ax.get_xlim()
        for c, l in zip(["#749FCD", "#DD654B", "#6BC865", "#D06CC0", "#C3B639", "#e8950e", "#d3d3d3"],
                        ["attC", "integrase", "Promoter/attI class 1",
                         "Promoter/attI class 2", "Promoter/attI class 3",
                         "Functional Annotation", "Hypothetical Protein"]):
            ax.bar(0, 0, color=c, label=l)
        plt.legend(loc=[1.01, 0.4])
        ax.set_xlim(xlims)
        fig.subplots_adjust(left=0.05, right=0.80)
        ax.hlines(0, ax.get_xlim()[0], ax.get_xlim()[1], "lightgrey", "--")
        ax.grid(True, "major", axis="x")
        ax.set_ylim(-4, 4)
        ax.get_yaxis().set_visible(False)
        if file != 0:
            fig.savefig(file, format="pdf")
            plt.close(fig)
        else:
            fig.show()


    def has_integrase(self):
        return len(self.integrase) >= 1


    def has_attC(self):
        return len(self.attC) >= 1


def search_attc(attc_df, keep_palindromes):
    """
    Parse the attc dataset (sorted along start site) for the given replicon and return list of arrays.
    One array is composed of attC sites on the same strand and separated by a
    distance less than 5kb
    """
    ok = 0

    position_bkp_minus = []
    position_bkp_plus = []

    attc_plus = attc_df[attc_df.sens == "+"].copy()
    attc_minus = attc_df[attc_df.sens == "-"].copy()

    if keep_palindromes == False:
        attc_df = attc_df.sort_values(["pos_beg", "evalue"]).drop_duplicates(subset=["pos_beg"]).copy()
        attc_plus = attc_df[attc_df.sens == "+"].copy()
        attc_minus = attc_df[attc_df.sens == "-"].copy()

    # can be reordered
    if (attc_plus.pos_beg.diff() > DISTANCE_THRESHOLD).any() or (attc_minus.pos_beg.diff() > DISTANCE_THRESHOLD).any():
        if len(attc_plus) > 0:
            bkp_plus = attc_plus[(attc_plus.pos_beg.diff() > DISTANCE_THRESHOLD)].index
            position_bkp_plus = [attc_plus.index.get_loc(i) for i in bkp_plus]
        if len(attc_minus) > 0:
            bkp_minus = attc_minus[(attc_minus.pos_beg.diff() > DISTANCE_THRESHOLD)].index
            position_bkp_minus = [attc_minus.index.get_loc(i) for i in bkp_minus]
        ok = 1

    if len(attc_plus) > 0 and len(attc_minus) > 0:
        ok = 1

    if not ok:
        if len(attc_df) == 0:
            return []
        else:
            return [attc_df]
    else:
        if len(attc_plus) > 0:
            array_plus = np.split(attc_plus.values, position_bkp_plus)
            if ((array_plus[0][0][4] - array_plus[-1][-1][4]) % SIZE_REPLICON <\
                 DISTANCE_THRESHOLD and len(array_plus) > 1):
                 array_plus[0] = np.concatenate((array_plus[-1], array_plus[0]))
                 del array_plus[-1]
        else:
            array_plus = np.array([])
        if len(attc_minus) > 0:
            array_minus = np.split(attc_minus.values, position_bkp_minus)
            if ((array_minus[0][0][4]-array_minus[-1][-1][4]) % SIZE_REPLICON <\
                 DISTANCE_THRESHOLD and len(array_minus) > 1):
                array_minus[0] = np.concatenate((array_minus[-1], array_minus[0]))
                del array_minus[-1]
        else:
            array_minus = np.array([])

        if len(array_minus) > 0 and len(array_plus) > 0:
            tmp = array_plus + array_minus
        elif len(array_minus) == 0:
            tmp = array_plus
        elif len(array_plus) == 0:
            tmp = array_minus

        attc_array = [pd.DataFrame(i, columns=["Accession_number", "cm_attC", "cm_debut",
                                               "cm_fin", "pos_beg", "pos_end", "sens", "evalue"]) for i in tmp]
        # convert positions to int, and evalue to float
        intcols = ["cm_debut", "cm_fin", "pos_beg", "pos_end"]
        for a in attc_array:
            a[intcols] = a[intcols].astype(int)
            a["evalue"] = a["evalue"].astype(float)
        return attc_array


def find_integron(replicon_name, attc_file, intI_file, phageI_file):
    """
    Function that looks for integrons given rules :
    - presence of intI
    - presence of attC
    - d(intI-attC) <= 4 kb
    - d(attC-attC) <= 4 kb
    It returns the list of all integrons, be they complete or not.
    found in attC files + integrases file which are formatted as follow :
    intI_file :
        Accession_number    ID_prot    strand    pos_beg    pos_end    evalue
    attc_file :
        Accession_number    attC    cm_debut    cm_fin    pos_beg    pos_end    sens    evalue

    :param replicon_name: the name of the replicon
    :type replicon_name: str
    :param attc_file: the output of cmsearch or the parsing of this file by read_infernal
    :type attc_file: file object or :class:`pd.Dataframe`
    :param intI_file: the output of hmmsearch with the integrase model
    :type intI_file: file object
    :param phageI_file: the output of hmmsearch with the phage model
    :type phageI_file: file object
    """
    if args.no_proteins == False:
        intI = read_hmm(replicon_name, intI_file)
        intI.sort_values(["Accession_number", "pos_beg", "evalue"], inplace=True)

        phageI = read_hmm(replicon_name, phageI_file)
        phageI.sort_values(["Accession_number", "pos_beg", "evalue"], inplace=True)

        tmp = intI[intI.ID_prot.isin(phageI.ID_prot)].copy()

        if len(tmp) >= 1:
            tmp.loc[:, "query_name"] = "intersection_tyr_intI"

        if args.union_integrases:
            intI_ac = intI[intI.ID_prot.isin(tmp.ID_prot) == 0
                          ].merge(phageI[phageI.ID_prot.isin(tmp.ID_prot) == 0],
                                  how="outer"
                                 ).merge(tmp, how="outer")
        else:
            intI_ac = tmp
    else:
        intI_ac = pd.DataFrame(columns=["Accession_number", "query_name", "ID_query",
                                        "ID_prot", "strand", "pos_beg", "pos_end",
                                        "evalue", "hmmfrom", "hmmto", "alifrom",
                                        "alito", "len_profile"])

    if isinstance(attc_file, pd.DataFrame):
        attc = attc_file
        attc.sort_values(["Accession_number", "pos_beg", "evalue"], inplace=True)

    else:
        attc = read_infernal(attc_file,
                             evalue=evalue_attc,
                             size_max_attc=max_attc_size,
                             size_min_attc=min_attc_size)
        attc.sort_values(["Accession_number", "pos_beg", "evalue"], inplace=True)

    attc_ac = search_attc(attc, args.keep_palindromes)  # list of Dataframe, each have an array of attC
    integrons = []

    if len(intI_ac) >= 1 and len(attc_ac) >= 1:
        n_attc_array = len(attc_ac)  # If an array hasn't been clustered with an Integrase
                                     # or if an integrase lacks an array
                                     # redontant info, we could check for len(attc_ac)==0
                                     # -> to remove
        for i, id_int in enumerate(intI_ac.ID_prot.values): #For each Integrase

            if n_attc_array == 0:  # No more array to attribute to an integrase

                integrons.append(Integron(replicon_name))
                integrons[-1].add_integrase(intI_ac.pos_beg.values[i],
                                       intI_ac.pos_end.values[i],
                                       id_int,
                                       int(intI_ac.strand.values[i]),
                                       intI_ac.evalue.values[i],
                                       intI_ac.query_name.values[i])

            else: # we still have attC and int :
                attc_left = np.array([i_attc.pos_beg.values[0] for i_attc in attc_ac])
                attc_right = np.array([i_attc.pos_end.values[-1] for i_attc in attc_ac])

                distances = np.array([(attc_left - intI_ac.pos_end.values[i]),
                                      (intI_ac.pos_beg.values[i] - attc_right)]) % SIZE_REPLICON

                if len(attc_ac) > 1:
                    #tmp = (distances /
                    #       np.array([[len(aac) for aac in attc_ac]]))

                    side, idx_attc = np.where((distances) == (distances).min())
                    # side : 0 <=> left; 1 <=> right
                    # index of the closest and biggest attC array to the integrase
                    # exactly tmp = dist(cluster to integrase) / size cluster
                    # to make a decision between 2 equally distant arrays
                    # Usually they are on the same side but on 2 different strands

                    # If they are exactly similar (same distance, same number of attC, take the first one arbitrarily
                    # Or just flatten from idx_attc=[i] to idx_attc=i
                    idx_attc = idx_attc[0]
                    side = side[0]

                else:
                    idx_attc = 0
                    side = np.argmin(distances)

                if distances[side, idx_attc] < DISTANCE_THRESHOLD:
                    integrons.append(Integron(replicon_name))
                    integrons[-1].add_integrase(intI_ac.pos_beg.values[i],
                                                intI_ac.pos_end.values[i],
                                                id_int,
                                                int(intI_ac.strand.values[i]),
                                                intI_ac.evalue.values[i],
                                                intI_ac.query_name.values[i])

                    attc_tmp = attc_ac.pop(idx_attc)

                    for a_tmp in attc_tmp.values:
                        integrons[-1].add_attC(a_tmp[4],
                                               a_tmp[5],
                                               1 if a_tmp[6] == "+" else -1,
                                               a_tmp[7], model_attc_name)
                    n_attc_array -= 1

                else: # no array close to the integrase on both side
                    integrons.append(Integron(replicon_name))
                    integrons[-1].add_integrase(intI_ac.pos_beg.values[i],
                                                intI_ac.pos_end.values[i],
                                                id_int,
                                                int(intI_ac.strand.values[i]),
                                                intI_ac.evalue.values[i], intI_ac.query_name.values[i])

        if n_attc_array > 0: # after the integrase loop (<=> no more integrases)
            for attc_array in attc_ac:
                integrons.append(Integron(replicon_name))

                for a_tmp in attc_array.values:
                    integrons[-1].add_attC(a_tmp[4],
                                           a_tmp[5],
                                           1 if a_tmp[6] == "+" else -1,
                                           a_tmp[7], model_attc_name)

    elif len(intI_ac.pos_end.values) == 0 and len(attc_ac) >= 1:  # If attC only
        for attc_array in attc_ac:
            integrons.append(Integron(replicon_name))
            for a_tmp in attc_array.values:
                integrons[-1].add_attC(a_tmp[4],
                                              a_tmp[5],
                                              1 if a_tmp[6] == "+" else -1,
                                              a_tmp[7], model_attc_name)

    elif len(intI_ac.pos_end.values) >= 1 and len(attc_ac) == 0: # If intI only
        for i, id_int in enumerate(intI_ac.ID_prot.values):
            integrons.append(Integron(replicon_name))
            integrons[-1].add_integrase(intI_ac.pos_beg.values[i],
                                       intI_ac.pos_end.values[i],
                                       id_int,
                                       int(intI_ac.strand.values[i]),
                                       intI_ac.evalue.values[i],
                                       intI_ac.query_name.values[i])

    print "In replicon {}, there are:".format(replicon_name)
    print "- {} complete integron(s) found with a total {} attC site(s)".format(sum(
                                                [1 if i.type() == "complete" else 0 for i in integrons]),
                                                sum([len(i.attC) if i.type() == "complete" else 0 for i in integrons]))
    print "- {} CALIN element(s) found with a total of {} attC site(s)".format(sum(
                                                [1 if i.type() == "CALIN" else 0 for i in integrons]),
                                                sum([len(i.attC) if i.type() == "CALIN" else 0 for i in integrons]))
    print "- {} In0 element(s) found with a total of {} attC site".format(sum(
                                                [1 if i.type() == "In0" else 0 for i in integrons]),
                                                sum([len(i.attC) if i.type() == "In0" else 0 for i in integrons]))

    return integrons


def find_attc_max(integrons, outfile="attC_max_1.res"):
    """
    Look for attC site with cmsearch --max option wich remove all heuristic filters.
    As this option make the algorithm way slower, we only run it in the region around a
    hit. We call it local_max or eagle_eyes.


    Default hit :
    =============
                     attC
    __________________-->____-->_________-->_____________
    ______<--------______________________________________
             intI
                  ^-------------------------------------^
                 Search-space with --local_max

    Updated hit :
    =============

                     attC          ***         ***
    __________________-->____-->___-->___-->___-->_______
    ______<--------______________________________________
             intI

    :param integrons: the integrons may contain or not attC or intI.
    :type integrons: list of :class:`Integron` objects.
    :param outfile: the name of cmsearch result file
    :type outfile: string
    :return:
    :rtype: :class:`pd.DataFrame`
    """
    columns = ['Accession_number', 'cm_attC', 'cm_debut', 'cm_fin', 'pos_beg', 'pos_end', 'sens', 'evalue']
    data_type = {'Accession_number': 'str', 'cm_attC': 'str',
                 'cm_debut': 'int', 'cm_fin': 'int',
                 'pos_beg': 'int', 'pos_end': 'int', }
    max_final = pd.DataFrame(columns=columns)
    max_final = max_final.astype(dtype=data_type)
    for i in integrons:
        max_elt = pd.DataFrame(columns=columns)
        max_elt = max_elt.astype(dtype=data_type)
        full_element = i.describe()

        if all(full_element.type == "complete"):

            # Where is the integrase compared to the attc sites (no matter the strand) :
            integrase_is_left = ((full_element[full_element.type_elt == "attC"].pos_beg.values[0] -
                                  full_element[full_element.annotation == "intI"
                                               ].pos_end.values[0]) % SIZE_REPLICON <
                                 (full_element[full_element.annotation == "intI"
                                               ].pos_beg.values[0] -
                                  full_element[full_element.type_elt == "attC"].pos_end.values[-1]) % SIZE_REPLICON)

            if integrase_is_left:
                window_beg = full_element[full_element.annotation == "intI"].pos_end.values[0]
                DISTANCE_THRESHOLD_LEFT = 0
                window_end = full_element[full_element.type_elt == "attC"].pos_end.values[-1]
                DISTANCE_THRESHOLD_RIGHT = DISTANCE_THRESHOLD

            else:  # is right
                window_beg = full_element[full_element.type_elt == "attC"].pos_beg.values[0]
                DISTANCE_THRESHOLD_LEFT = DISTANCE_THRESHOLD
                window_end = full_element[full_element.annotation == "intI"].pos_end.values[-1]
                DISTANCE_THRESHOLD_RIGHT = 0

            if circular:
                window_beg = (window_beg - DISTANCE_THRESHOLD_LEFT) % SIZE_REPLICON
                window_end = (window_end + DISTANCE_THRESHOLD_RIGHT) % SIZE_REPLICON
            else:
                window_beg = max(0, window_beg - DISTANCE_THRESHOLD_LEFT)
                window_end = min(SIZE_REPLICON, window_end + DISTANCE_THRESHOLD_RIGHT)

            strand = "top" if full_element[full_element.type_elt == "attC"].strand.values[0] == 1 else "bottom"
            df_max = local_max(replicon_name, window_beg, window_end, strand)
            max_elt = pd.concat([max_elt, df_max])

            # If we find new attC after the last found with default algo and if the integrase is on the left
            # (We don't expand over the integrase) :
            # pos_beg - pos_end so it's the same, the distance will always be > DISTANCE_THRESHOLD

            go_left = (full_element[full_element.type_elt == "attC"].pos_beg.values[0] - df_max.pos_end.values[0]
                       ) % SIZE_REPLICON < DISTANCE_THRESHOLD and not integrase_is_left
            go_right = (df_max.pos_beg.values[-1] - full_element[full_element.type_elt == "attC"].pos_end.values[-1]
                        ) % SIZE_REPLICON < DISTANCE_THRESHOLD and integrase_is_left
            max_elt = expand(window_beg, window_end, max_elt, df_max,
                             search_left=go_left, search_right=go_right)

        elif all(full_element.type == "CALIN"):
            if len(full_element[full_element.pos_beg.isin(max_final.pos_beg)]) == 0:
                # if cluster don't overlap already max-searched region
                window_beg = full_element[full_element.type_elt == "attC"].pos_beg.values[0]
                window_end = full_element[full_element.type_elt == "attC"].pos_end.values[-1]
                if circular:
                    window_beg = (window_beg - DISTANCE_THRESHOLD) % SIZE_REPLICON
                    window_end = (window_end + DISTANCE_THRESHOLD) % SIZE_REPLICON
                else:
                    window_beg = max(0, window_beg - DISTANCE_THRESHOLD)
                    window_end = min(SIZE_REPLICON, window_end + DISTANCE_THRESHOLD)
                strand = "top" if full_element[full_element.type_elt == "attC"].strand.values[0] == 1 else "bottom"
                df_max = local_max(replicon_name, window_beg, window_end, strand)
                max_elt = pd.concat([max_elt, df_max])

                if len(df_max) > 0: # Max can sometimes find bigger attC than permitted
                    go_left = (full_element[full_element.type_elt == "attC"].pos_beg.values[0] - df_max.pos_end.values[0]
                               ) % SIZE_REPLICON < DISTANCE_THRESHOLD
                    go_right = (df_max.pos_beg.values[-1] - full_element[full_element.type_elt == "attC"].pos_end.values[-1]
                                ) % SIZE_REPLICON < DISTANCE_THRESHOLD
                    max_elt = expand(window_beg, window_end, max_elt, df_max,
                                     search_left=go_left, search_right=go_right)

        elif all(full_element.type == "In0"):
            if all(full_element.model != "Phage_integrase"):
                window_beg = full_element[full_element.annotation == "intI"].pos_beg.values[0]
                window_end = full_element[full_element.annotation == "intI"].pos_end.values[-1]
                if circular:
                    window_beg = (window_beg - DISTANCE_THRESHOLD) % SIZE_REPLICON
                    window_end = (window_end + DISTANCE_THRESHOLD) % SIZE_REPLICON
                else:
                    window_beg = max(0, window_beg - DISTANCE_THRESHOLD)
                    window_end = min(SIZE_REPLICON, window_end + DISTANCE_THRESHOLD)
                df_max = local_max(replicon_name, window_beg, window_end)
                max_elt = pd.concat([max_elt, df_max])
                if len(max_elt) > 0:
                    max_elt = expand(window_beg, window_end, max_elt, df_max,
                                     search_left=True, search_right=True)

        max_final = pd.concat([max_final, max_elt])
        max_final.drop_duplicates(subset=max_final.columns[:-1], inplace=True)
        max_final.index = range(len(max_final))
    return max_final


def expand(window_beg, window_end, max_elt, df_max, search_left=False, search_right=False):
    """
    for a given element, we can search on the left hand side (if integrase is on the right for instance)
    or right hand side (opposite situation) or both side (only integrase or only attC sites)

    :param window_beg:
    :type window_beg: int
    :param window_end:
    :type window_end: int
    :param max_elt:
    :type max_elt: int
    :param df_max:
    :type df_max: :class:`pandas.DataFrame` object
    :param search_left: need to search on right of ???
    :type search_left: bool
    :param search_right: need to search on right of ???
    :type search_right: bool
    :return: a copy of max_elt with attC hits
    :rtype: :class:`pandas.DataFrame` object
    """
    # for a given element, we can search on the left hand side (if integrase is on the right for instance)
    # or right hand side (opposite situation) or both side (only integrase or only attC sites)
    wb = window_beg
    we = window_end

    if search_right:

        if circular:
            window_beg = (window_end - max_attc_size) % SIZE_REPLICON
            # max_attc_size (200bo by default) to allow the detection of sites that would overlap 2 consecutive windows
            window_end = (window_end + DISTANCE_THRESHOLD) % SIZE_REPLICON
        else:
            window_beg = max(0, window_end - max_attc_size)
            # max_attc_size (200bo by default) to allow the detection of sites that would overlap 2 consecutive windows
            window_end = min(SIZE_REPLICON, window_end + DISTANCE_THRESHOLD)

        searched_strand = "both" if search_left else "top" # search on both strands if search in both directions

        while len(df_max) > 0 and 0 < (window_beg and window_end) < SIZE_REPLICON:

            df_max = local_max(replicon_name, window_beg, window_end, searched_strand)
            max_elt = pd.concat([max_elt, df_max])

            if circular:
                window_beg = (window_end - max_attc_size) % SIZE_REPLICON
                window_end = (window_end + DISTANCE_THRESHOLD) % SIZE_REPLICON
            else:
                window_beg = max(0, window_end - max_attc_size)
                window_end = min(SIZE_REPLICON, window_end + DISTANCE_THRESHOLD)

        # re-initialize in case we enter search left too.
        df_max = max_elt.copy()
        window_beg = wb
        window_end = we

    if search_left:

        if circular:
            window_end = (window_beg + 200) % SIZE_REPLICON
            window_beg = (window_beg - DISTANCE_THRESHOLD) % SIZE_REPLICON

        else:
            window_beg = max(0, window_beg - DISTANCE_THRESHOLD)
            window_end = min(SIZE_REPLICON, window_beg + 200)

        searched_strand = "both" if search_right else "bottom"

        while len(df_max) > 0 and 0 < (window_beg and window_end) < SIZE_REPLICON:

            df_max = local_max(replicon_name, window_beg, window_end, searched_strand)
            max_elt = pd.concat([max_elt, df_max])  # update of attC list of hits.

            if circular:
                window_end = (window_beg + 200) % SIZE_REPLICON
                window_beg = (window_beg - DISTANCE_THRESHOLD) % SIZE_REPLICON

            else:
                window_end = min(SIZE_REPLICON, window_beg + 200)
                window_beg = max(0, window_beg - DISTANCE_THRESHOLD)

    max_elt.drop_duplicates(inplace=True)
    max_elt.index = range(len(max_elt))
    return max_elt


def local_max(replicon_name, window_beg, window_end, strand_search="both"):
    """

    :param replicon_name: the name of replicon (without suffix)
    :type replicon_name: str
    :param window_beg:
    :type window_beg: int
    :param window_end:
    :type window_end: int
    :param strand_search:
    :type strand_search: str
    :return:
    :rtype: :class:`pd.DataFrame` object
    """
    if window_beg < window_end:
        subseq = SEQUENCE[window_beg : window_end]
    else:
        subseq1 = SEQUENCE[window_beg : SIZE_REPLICON]
        subseq2 = SEQUENCE[:window_end]
        subseq = subseq1 + subseq2

    with open(os.path.join(out_dir, replicon_name + "_subseq.fst"), "w") as f:
        SeqIO.write(subseq, f, "fasta")

    output_path = os.path.join(out_dir,
                               "{name}_{win_beg}_{win_end}_subseq_attc.res".format(name=replicon_name,
                                                                                    win_beg=window_beg,
                                                                                    win_end=window_end))
    tblout_path = os.path.join(out_dir,
                                "{name}_{win_beg}_{win_end}_subseq_attc_table.res".format(name=replicon_name,
                                                                                          win_beg=window_beg,
                                                                                          win_end=window_end))

    infile_path = os.path.join(out_dir, replicon_name + "_subseq.fst")
    if strand_search == "both":
        cmsearch_cmd = [CMSEARCH,
                        "-Z", str(SIZE_REPLICON / 1000000.),
                        "--max",
                        "--cpu", N_CPU,
                        "-o", output_path,
                        "--tblout", tblout_path,
                        "-E", "10",
                        MODEL_attc,
                        infile_path]

    elif strand_search == "top":
        cmsearch_cmd = [CMSEARCH,
                        "-Z", str(SIZE_REPLICON / 1000000.),
                        "--toponly",
                        "--max",
                        "--cpu", N_CPU,
                        "-o", output_path,
                        "--tblout", tblout_path,
                        "-E", "10",
                        MODEL_attc,
                        infile_path]

    elif strand_search == "bottom":
        cmsearch_cmd = [CMSEARCH,
                        "-Z", str(SIZE_REPLICON / 1000000.),
                        "--bottomonly",
                        "--max",
                        "--cpu", N_CPU,
                        "-o", output_path,
                        "--tblout", tblout_path,
                        "-E", "10",
                        MODEL_attc,
                        infile_path]
    try:
        returncode = call(cmsearch_cmd)
    except Exception as err:
        raise RuntimeError("{0} failed : {1}".format(cmsearch_cmd[0], err))
    if returncode != 0:
        raise RuntimeError("{0} failed returncode = {1}".format(cmsearch_cmd[0], returncode))

    df_max = read_infernal(tblout_path,
                           evalue=evalue_attc,
                           size_max_attc=max_attc_size,
                           size_min_attc=min_attc_size)
    df_max.pos_beg = (df_max.pos_beg + window_beg) % SIZE_REPLICON
    df_max.pos_end = (df_max.pos_end + window_beg) % SIZE_REPLICON
    df_max.to_csv(os.path.join(out_dir, replicon_name + "_subseq_attc_table_end.res"),
                  sep="\t", index=0, mode="a", header=0)
    # filter on size
    df_max = df_max[(abs(df_max.pos_end - df_max.pos_beg) > min_attc_size) & (abs(df_max.pos_end - df_max.pos_beg) < max_attc_size)]
    return df_max


def find_attc(replicon_path, replicon_name, out_dir):
    """
    Call cmsearch to find attC sites in a single replicon.

    :param replicon_path: the path of the replicon to analyse
    :type replicon_path: string
    :param replicon_name: the name of the replicon to analyse
    :type replicon_name: string
    :param out_dir: the relative path to the directory where cmsearch outputs will be stored
    :type out_dir: str
    :returns: None, the results are written on the disk
    :raises RuntimeError: when cmsearch run failed
    """
    cmsearch_cmd = [CMSEARCH,
                    "--cpu", N_CPU,
                    "-o", os.path.join(out_dir, replicon_name + "_attc.res"),
                    "--tblout", os.path.join(out_dir, replicon_name + "_attc_table.res"),
                    "-E", "10",
                    MODEL_attc,
                    replicon_path]
    try:
        returncode = call(cmsearch_cmd)
    except Exception as err:
        raise RuntimeError("{0} failed : {1}".format(cmsearch_cmd[0], err))
    if returncode != 0:
        raise RuntimeError("{0} failed returncode = {1}".format(cmsearch_cmd[0], returncode))


def find_integrase(replicon_path, replicon_name, out_dir):
    """
    Call Prodigal for Gene annotation and hmmer to find integrase, either with phage_int
    HMM profile or with intI profile.

    :param replicon_path: the path of the replicon to analyse
    :type replicon_path: string
    :param replicon_name: the name of the replicon to analyse
    :type replicon_name: string
    :param out_dir: the relative path to the directory where prodigal outputs will be stored
    :type out_dir: str
    :returns: None, the results are written on the disk
    """
    if not args.gembase:
        # Test whether the protein file exist to avoid new annotation for each run on the same replicon
        prot_tr_path = os.path.join(out_dir, replicon_name + ".prt")
        if not os.path.isfile(prot_tr_path):
            dev_null = 'NUL' if platform.system() == 'Windows' else '/dev/null'
            if SIZE_REPLICON > 200000:
                prodigal_cmd = [PRODIGAL,
                                "-i", replicon_path,
                                "-a", prot_tr_path,
                                "-o", dev_null]

            else: # if small genome, prodigal annotate it as contig.
                prodigal_cmd = [PRODIGAL,
                                "-p", "meta",
                                "-i", replicon_path,
                                "-a", prot_tr_path,
                                "-o", dev_null]
            try:
                returncode = call(prodigal_cmd)
            except Exception as err:
                raise RuntimeError("{0} failed : {1}".format(prodigal_cmd[0], err))
            if returncode != 0:
                raise RuntimeError("{0} failed returncode = {1}".format(prodigal_cmd[0], returncode))

    intI_hmm_out = os.path.join(out_dir, replicon_name + "_intI.res")
    hmm_cmd = []
    if not os.path.isfile(intI_hmm_out):
        hmm_cmd.append([HMMSEARCH,
                        "--cpu", N_CPU,
                        "--tblout", os.path.join(out_dir, replicon_name + "_intI_table.res"),
                        "-o", intI_hmm_out,
                        MODEL_integrase,
                        PROT_file])

    phage_hmm_out = os.path.join(out_dir, replicon_name + "_phage_int.res")
    if not os.path.isfile(phage_hmm_out):
        hmm_cmd.append([HMMSEARCH,
                        "--cpu", N_CPU,
                        "--tblout", os.path.join(out_dir, replicon_name + "_phage_int_table.res"),
                        "-o", phage_hmm_out,
                        MODEL_phage_int,
                        PROT_file])

    for cmd in hmm_cmd:
        try:
            returncode = call(cmd)
        except Exception as err:
            raise RuntimeError("{0} failed : {1}".format(' '.join(cmd), err))
        if returncode != 0:
            raise RuntimeError("{0} failed return code = {1}".format(' '.join(cmd), returncode))


def func_annot(replicon_name, out_dir, hmm_files, evalue=10, coverage=0.5):
    """
    Call hmmmer to annotate CDS associated with the integron. Use Resfams per default (Gibson et al, ISME J.,  2014)
    """
    print "# Start Functional annotation... : "
    prot_tmp = os.path.join(out_dir, replicon_name + "_subseqprot.tmp")

    for integron in integrons:
        if os.path.isfile(prot_tmp):
            os.remove(prot_tmp)

        if integron.type() != "In0" and len(integron.proteins) > 0:

            func_annotate_res = pd.DataFrame(columns=["Accession_number",
                                                      "query_name", "ID_query",
                                                      "ID_prot", "strand",
                                                      "pos_beg", "pos_end", "evalue"])


            prot_to_annotate = []
            prot = SeqIO.parse(PROT_file, "fasta")
            n_prot = 0
            for p in prot:
                n_prot += 1
                if p.id in integron.proteins.index:
                    prot_to_annotate.append(p)
            SeqIO.write(prot_to_annotate, prot_tmp, "fasta")
            for hmm in hmm_files:
                hmm_out = os.path.join(out_dir, "_".join([replicon_name,
                                                         hmm.split("/")[-1].split(".")[0],
                                                         "fa.res"]))
                hmm_tableout = os.path.join(out_dir, "_".join([replicon_name,
                                                         hmm.split("/")[-1].split(".")[0],
                                                         "fa_table.res"]))
                hmm_cmd = [HMMSEARCH,
                            "-Z", str(n_prot),
                            "--cpu", N_CPU,
                            "--tblout", hmm_tableout,
                            "-o", hmm_out,
                            hmm,
                            prot_tmp]

                try:
                    returncode = call(hmm_cmd)
                except Exception as err:
                    raise RuntimeError("{0} failed : {1}".format(hmm_cmd[0], err))
                if returncode != 0:
                    raise RuntimeError("{0} failed return code = {1}".format(hmm_cmd[0], returncode))
                hmm_in = read_hmm(replicon_name, hmm_out, evalue=evalue, coverage=coverage).sort_values("evalue").drop_duplicates(subset="ID_prot")
                func_annotate_res = pd.concat([func_annotate_res, hmm_in])
            func_annotate_res = func_annotate_res.sort_values("evalue").drop_duplicates(subset="ID_prot")

            integron.proteins.loc[func_annotate_res.ID_prot, "evalue"] = func_annotate_res.evalue.values
            integron.proteins.loc[func_annotate_res.ID_prot, "annotation"] = func_annotate_res.query_name.values
            integron.proteins.loc[func_annotate_res.ID_prot, "model"] = func_annotate_res.ID_query.values
            integron.proteins = integron.proteins.astype(dtype=integron.dtype)


def read_hmm(replicon_name, infile, evalue=1, coverage=0.5):
    """
    Function that parse hmmer --out output and returns a pandas DataFrame
    filter output by evalue and coverage. (Being % of the profile aligned)
    """

    df = pd.DataFrame(columns=["Accession_number", "query_name", "ID_query",
                               "ID_prot", "strand", "pos_beg", "pos_end",
                               "evalue", "hmmfrom", "hmmto", "alifrom",
                               "alito", "len_profile"])
    gen = SearchIO.parse(infile, 'hmmer3-text')
    for idx, query_result in enumerate(gen):
        len_profile = query_result.seq_len
        query = query_result.id

        try:
            id_query = query_result.accession
        except AttributeError:
            id_query = "-"
        for idx2, hit in enumerate(query_result.hits):
            id_prot = hit.id
            if args.gembase == False:
                pos_beg, pos_end, strand = [x.strip() for x in hit.description_all[0].split('#') if x][:-1]
            else:
                desc = [j for j in hit.description_all[0].split(" ")]
                strand = 1 if desc[0] == "D" else -1
                pos_beg = int(desc[3])
                pos_end = int(desc[4])

            evalue_tmp = []
            hmmfrom = []
            hmmto = []
            alifrom = []
            alito = []

            for hsp in hit.hsps:
                #strand = hsp.query_strand
                evalue_tmp.append(hsp.evalue)
                hmmfrom.append(hsp.query_start + 1)
                hmmto.append(hsp.query_end)
                alifrom.append(hsp.hit_start + 1)
                alito.append(hsp.hit_end)

            best_evalue = np.argmin(evalue_tmp)

            df.loc[idx+idx2, "ID_prot"] = id_prot
            df.loc[idx+idx2, "ID_query"] = id_query # "-"  # remnant of ancient parsing function to keep data structure
            df.loc[idx+idx2, "pos_beg"] = int(pos_beg)
            df.loc[idx+idx2, "pos_end"] = int(pos_end)
            df.loc[idx+idx2, "strand"] = int(strand)
            df.loc[idx+idx2, "evalue"] = evalue_tmp[best_evalue]   # i-evalue
            df.loc[idx+idx2, "hmmfrom"] = hmmfrom[best_evalue]   # hmmfrom
            df.loc[idx+idx2, "hmmto"] = hmmto[best_evalue]     # hmm to
            df.loc[idx+idx2, "alifrom"] = alifrom[best_evalue]   # alifrom
            df.loc[idx+idx2, "alito"] = alito [best_evalue]  # ali to
            df.loc[idx+idx2, "len_profile"] = float(len_profile)
            df.loc[idx+idx2, "Accession_number"] = replicon_name
            df.loc[idx+idx2, "query_name"] = query

    intcols = ["pos_beg", "pos_end", "strand"]
    floatcol = ["evalue", "len_profile"]
    df[intcols] = df[intcols].astype(int)
    df[floatcol] = df[floatcol].astype(float)

    df = df[((df.hmmto - df.hmmfrom) / df.len_profile > coverage) & (df.evalue < evalue)]
    df.index = range(len(df))

    df_out = df[["Accession_number", "query_name", "ID_query", "ID_prot",
                 "strand", "pos_beg", "pos_end", "evalue"]]

    return df_out


def read_infernal(infile, evalue=1, size_max_attc=200, size_min_attc=40):
    """
    Function that parse cmsearch --tblout output and returns a pandas DataFrame
    """

    try:
        _ = pd.read_table(infile, comment="#")
    except:
        return pd.DataFrame(columns=["Accession_number", "cm_attC", "cm_debut",
                                     "cm_fin", "pos_beg", "pos_end", "sens", "evalue"])

    df = pd.read_table(infile, sep="\s*", engine="python",
                       header=None, skipfooter=10, skiprows=2)
    # Keep only columns: query_name(2), mdl from(5), mdl to(6), seq from(7),
    # seq to(8), strand(9), E-value(15)
    df = df[[2, 5, 6, 7, 8, 9, 15]]
    df = df[(df[15] < evalue)]  # filter on evalue
    df = df[(abs(df[8]-df[7]) < size_max_attc) & (size_min_attc < abs(df[8]-df[7]))]
    if len(df) > 0:
        df["Accession_number"] = replicon_name
        c = df.columns.tolist()
        df = df[c[-1:] + c[:-1]]
        df.sort_values([8, 15], inplace=True)
        df.index = range(0, len(df))
        df.columns = ["Accession_number", "cm_attC", "cm_debut", "cm_fin",
                      "pos_beg_tmp", "pos_end_tmp",
                      "sens", "evalue"]
        idx = (df.pos_beg_tmp > df.pos_end_tmp)
        df.loc[idx, "pos_beg"] = df.loc[idx].apply(lambda x: x["pos_end_tmp"] - (length_cm - x["cm_fin"]), axis=1)
        df.loc[idx, "pos_end"] = df.loc[idx].apply(lambda x: x["pos_beg_tmp"] + (x["cm_debut"] - 1), axis=1)

        df.loc[~idx, "pos_end"] = df.loc[~idx].apply(lambda x: x["pos_end_tmp"] + (length_cm - x["cm_fin"]), axis=1)
        df.loc[~idx, "pos_beg"] = df.loc[~idx].apply(lambda x: x["pos_beg_tmp"] - (x["cm_debut"] - 1), axis=1)

        return df[["Accession_number", "cm_attC", "cm_debut", "cm_fin",
                      "pos_beg", "pos_end",
                      "sens", "evalue"]]
    else:
        return pd.DataFrame(columns=["Accession_number", "cm_attC", "cm_debut",
                                     "cm_fin", "pos_beg", "pos_end", "sens", "evalue"])



def to_gbk(df, sequence):

    """ from a dataframe like integrons_describe and a sequence, create an genbank file with integron annotation """

    df = df.set_index("ID_integron").copy()
    for i in df.index.unique():

        if isinstance(df.loc[i], pd.Series):
            type_integron = df.loc[i].type
            start_integron = df.loc[i].pos_beg
            end_integron = df.loc[i].pos_end
            tmp = SeqFeature.SeqFeature(location=
               SeqFeature.FeatureLocation(start_integron - 1,
                                         end_integron),
               strand=0,
               type="integron",
               qualifiers={"integron_id" : i,
                           "integron_type" : type_integron}
               )
            sequence.features.append(tmp)
            if df.loc[i].type_elt == "protein":

                tmp = SeqFeature.SeqFeature(location=
                                       SeqFeature.FeatureLocation(df.loc[i].pos_beg - 1,
                                                                 df.loc[i].pos_end),
                                       strand=df.loc[i].strand,
                                       type="CDS" if df.loc[i].annotation != "intI" else "integrase",
                                       qualifiers={"protein_id" : df.loc[i].element,
                                                  "gene" : df.loc[i].annotation,
                                                  "model" : df.loc[i].model}
                                       )

                tmp.qualifiers["translation"] = [prt for prt in SeqIO.parse(PROT_file, "fasta")
                                                 if prt.id == df.loc[i].element][0].seq
                sequence.features.append(tmp)


            else:
                tmp = SeqFeature.SeqFeature(location=
                                   SeqFeature.FeatureLocation(df.loc[i].pos_beg - 1,
                                                             df.loc[i].pos_end),
                                   strand=df.loc[i].strand,
                                   type=df.loc[i].type_elt,
                                   qualifiers={df.loc[i].type_elt :df.loc[i].element,
                                               "model" : df.loc[i].model}
                                   )

                sequence.features.append(tmp)

        else:
            type_integron = df.loc[i].type.values[0]
            # Should only be true if integron over edge of sequence:
            diff = df.loc[i].pos_beg.diff() > DISTANCE_THRESHOLD

            if diff.any():
                pos = np.where(diff)[0][0]

                start_integron_1 = df.loc[i].pos_beg.values[pos]

                end_integron_1 = SIZE_REPLICON

                start_integron_2 = 1

                end_integron_2 = df.loc[i].pos_end.values[pos-1]

                f1 = SeqFeature.FeatureLocation(start_integron_1 - 1, end_integron_1)
                f2 = SeqFeature.FeatureLocation(start_integron_2 - 1, end_integron_2)
                tmp = SeqFeature.SeqFeature(location=f1 + f2,
                                            strand=0,
                                            type="integron",
                                            qualifiers={"integron_id" : i,
                                                        "integron_type" : type_integron}
                                            )
            else:
                start_integron = df.loc[i].pos_beg.values[0]
                end_integron = df.loc[i].pos_end.values[-1]

                tmp = SeqFeature.SeqFeature(location=
                               SeqFeature.FeatureLocation(start_integron - 1,
                                                          end_integron),
                                                          strand=0,
                                                          type="integron",
                                                          qualifiers={"integron_id" : i,
                                                          "integron_type" : type_integron}
                               )
            sequence.features.append(tmp)
            for r in df.loc[i].iterrows():
                if r[1].type_elt == "protein":
                    tmp = SeqFeature.SeqFeature(location=
                                           SeqFeature.FeatureLocation(r[1].pos_beg - 1,
                                                                      r[1].pos_end),
                                                                      strand=r[1].strand,
                                                                      type="CDS" if r[1].annotation != "intI" else "integrase",
                                                                      qualifiers={"protein_id" : r[1].element,
                                                                      "gene" : r[1].annotation,
                                                                      "model" : r[1].model}
                                                                     )

                    tmp.qualifiers["translation"] = [prt for prt in SeqIO.parse(PROT_file, "fasta")
                                                     if prt.id == r[1].element][0].seq
                    sequence.features.append(tmp)
                else:
                    tmp = SeqFeature.SeqFeature(location=
                                       SeqFeature.FeatureLocation(r[1].pos_beg - 1,
                                                                 r[1].pos_end),
                                       strand=r[1].strand,
                                       type=r[1].type_elt,
                                       qualifiers={r[1].type_elt :r[1].element,
                                                   "model" : r[1].model}
                                   )

                    sequence.features.append(tmp)

    # We get a ValueError otherwise, eg:
    # ValueError: Locus identifier 'gi|00000000|gb|XX123456.2|' is too long
    if len(sequence.name) > 16:
        sequence.name = sequence.name[-16:]


def scan_hmm_bank(path):
    """
    :param path: - if the path is a dir:
                   return all files ending with .hmm in the dir
                 - if the path is a file:
                   parse the file, each line must be an expression (glob)
                   pointing to hmm files
    :return: lists of hmm files to consider for annotation
    """
    real_path = os.path.realpath(path)
    files = []
    if os.path.exists(real_path):
        if os.path.isdir(real_path):
            files = glob.glob(os.path.join(real_path, '*.hmm'))
        elif os.path.isfile(real_path):
            with open(real_path) as hmm_bank:
                for bank_path in hmm_bank:
                    if bank_path.startswith('#'):
                        continue
                    elif not os.path.isabs(bank_path):
                        if "_prefix_share" in globals():
                            prefix = _prefix_share
                        else:
                            prefix = os.environ['INTEGRON_HOME']
                        bank_path = os.path.normpath(os.path.join(prefix, bank_path))
                    bank_files = glob.glob(os.path.expanduser(bank_path.strip("\n").strip()))
                    if not bank_files:
                        print >> sys.stderr, "WARNING func_annot '{}' does not match any files.".format(bank_path)
                    else:
                        for path in bank_files:
                            print >> sys.stderr, "the hmm {} will be used for functional annotation".format(path)
                        files.extend(bank_files)

        return files
    else:
        raise IOError("{} no such file or directory".format(path))

if __name__ == "__main__":

    ############### setup ###############

    _prefix_share = '$PREFIXSHARE'

    # integron was not installed using the setup.py
    # it's a developement version using environment variable
    if 'INTEGRON_HOME' in os.environ and os.environ['INTEGRON_HOME']:
        _prefix_share = os.environ['INTEGRON_HOME']

    _prefix_data = os.path.join(_prefix_share, 'data')

    if not os.path.exists(_prefix_data):
        raise Exception("""cannot find integron_finder data check your installation
or define INTEGRON_HOME environment variable.""")


    def get_version_message():
        import sys
        # if week keep '$VERSION' as is
        # the setup.py will replace it by the value set in setup
        # so the test become True even if integron_finder is installed using setup.py
        if __version__ == '$' + 'VERSION':
            version = "NOT packaged, it should be development version"
        else:
            version = __version__
        version_text = """integron_finder version {0}
Python {1}

 - open-source GPLv3,
 - Jean Cury, Bertrand Neron, Eduardo Rocha,
 - citation:

 Identification and analysis of integrons and cassette arrays in bacterial genomes
 Jean Cury; Thomas Jove; Marie Touchon; Bertrand Neron; Eduardo PC Rocha
 Nucleic Acids Research 2016; doi: 10.1093/nar/gkw319
 """.format(version, sys.version)
        return version_text

    ############### Arguments and declarations ###############
    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument("replicon",
                        help="Path to the replicon file (in fasta format), eg : path/to/file.fst or file.fst")

    parser.add_argument("--local_max",
                        help="Allows thorough local detection (slower but more sensitive and do not increase false positive rate).",
                        action="store_true")

    parser.add_argument("--func_annot",
                        help="Functional annotation of CDS associated with integrons HMM files are needed in Func_annot folder.",
                        default= False,
                        action="store_true")

    parser.add_argument('--cpu',
                        default='1',
                        action='store',
                        type=str,
                        help='Number of CPUs used by INFERNAL and HMMER')

    parser.add_argument('-dt', '--distance_thresh',
                        default=4000,
                        action='store',
                        type=int,
                        help='Two elements are aggregated if they are distant of DISTANCE_THRESH [4kb] or less')

    parser.add_argument('--outdir',
                        default=".",
                        action='store',
                        type=str,
                        metavar='.',
                        help='Set the output directory (default: current)')

    parser.add_argument("--linear",
                        help="Consider replicon as linear. If replicon smaller than 20kb, it will be considered as linear",
                        action="store_true")

    parser.add_argument("--union_integrases",
                        help="Instead of taking intersection of hits from Phage_int profile (Tyr recombinases) and integron_integrase profile, use the union of the hits",
                        action="store_true")

    parser.add_argument('--cmsearch',
                        default=distutils.spawn.find_executable("cmsearch"),
                        action='store',
                        type=str,
                        help='Complete path to cmsearch if not in PATH. eg: /usr/local/bin/cmsearch')

    parser.add_argument('--hmmsearch',
                        default=distutils.spawn.find_executable("hmmsearch"),
                        action='store',
                        type=str,
                        help='Complete path to hmmsearch if not in PATH. eg: /usr/local/bin/hmmsearch')

    parser.add_argument('--prodigal',
                        default=distutils.spawn.find_executable("prodigal"),
                        action='store',
                        type=str,
                        help='Complete path to prodigal if not in PATH. eg: /usr/local/bin/prodigal')

    parser.add_argument('--path_func_annot',
                        action='store',
                        metavar='bank_hmm',
                        type=str,
                        help='Path to file containing all hmm bank paths (one per line)')

    parser.add_argument("--gembase",
                        help="Use gembase formatted protein file instead of Prodigal. Folder structure must be preserved",
                        action="store_true")

    parser.add_argument('--attc_model',
                        default='attc_4.cm',
                        action='store',
                        type=str,
                        metavar='file.cm',
                        help='path or file to the attc model (Covariance Matrix)')

    parser.add_argument('--evalue_attc',
                        default=1.,
                        action='store',
                        type=float,
                        metavar='1',
                        help='set evalue threshold to filter out hits above it (default: 1)')

    parser.add_argument("--keep_palindromes",
                        help="for a given hit, if the palindromic version is found, don't remove the one with highest evalue ",
                        action="store_true")

    parser.add_argument("--no_proteins",
                        help="Don't annotate CDS and don't find integrase, just look for attC sites.",
                        action="store_true")

    parser.add_argument('--max_attc_size',
                        default=200,
                        action='store',
                        type=int,
                        metavar='200',
                        help='set maximum value fot the attC size (default: 200bp)')

    parser.add_argument('--min_attc_size',
                        default=40,
                        action='store',
                        type=int,
                        metavar='40',
                        help='set minimum value fot the attC size (default: 40bp)')

    parser.add_argument("--eagle_eyes",
                        help="Synonym of --local_max. Like a soaring eagle in the sky, catching rabbits(or attC sites) by surprise.",
                        action="store_true")

    parser.add_argument("-V", "--version",
                        action="version",
                        version=get_version_message())

    args = parser.parse_args()

    replicon_path = os.path.abspath(args.replicon)
    evalue_attc = args.evalue_attc

    in_dir, sequence_file = os.path.split(replicon_path)
    replicon_name, extension = os.path.splitext(sequence_file)
    in_dir = os.path.abspath(in_dir)

    mode_name = "local_max" if (args.eagle_eyes or args.local_max) else "default"

    try:
        os.mkdir(args.outdir)
    except OSError:
        pass

    try:
        os.mkdir(os.path.join(args.outdir, "Results_Integron_Finder_" + replicon_name))
    except OSError:
        pass

    try:
        os.mkdir(os.path.join(args.outdir,
                              "Results_Integron_Finder_" + replicon_name,
                              "other"))
    except OSError:
        pass

    out_dir = os.path.join(args.outdir,
                           "Results_Integron_Finder_" + replicon_name,
                           "other")
    out_dir_ok = os.path.join(args.outdir,
                              "Results_Integron_Finder_" + replicon_name)

    SEQUENCE = SeqIO.read(replicon_path,
                          "fasta",
                          alphabet=Seq.IUPAC.unambiguous_dna)

    ############### Definitions ###############

    N_CPU = args.cpu
    SIZE_REPLICON = len(SEQUENCE)
    DISTANCE_THRESHOLD = args.distance_thresh

    # If sequence is too small, it can be problematic when using circularity
    if len(SEQUENCE) > 4 * DISTANCE_THRESHOLD:
        circular = not args.linear
    else:
        circular = False

    MODEL_DIR = os.path.join(_prefix_data, "Models/")
    MODEL_integrase = os.path.join(MODEL_DIR, "integron_integrase.hmm")
    MODEL_phage_int = os.path.join(MODEL_DIR, "phage-int.hmm")

    CMSEARCH = args.cmsearch
    if CMSEARCH is None:
        raise RuntimeError("""cannot find 'cmsearch' in your PATH.
Please install infernal package or setup 'cmsearch' binary path with --cmsearch option""")
    HMMSEARCH = args.hmmsearch
    if HMMSEARCH is None:
        raise RuntimeError("""cannot find 'hmmsearch' in your PATH.
Please install hmmer package or setup 'hmmsearch' binary path with --hmmsearch option""")
    PRODIGAL = args.prodigal
    if PRODIGAL is None:
        raise RuntimeError("""cannot find 'prodigal' in your PATH.
Please install prodigal package or setup 'prodigal' binary path with --prodigal option""")

    if args.func_annot and args.no_proteins is False:
        fa_path = os.path.join(_prefix_data, "Functional_annotation")
        if os.path.exists('bank_hmm'):
            FA_HMM = scan_hmm_bank('bank_hmm')
        elif os.path.exists(fa_path):
            FA_HMM = scan_hmm_bank(fa_path)
        else:
            raise IntegronError("the dir '{}' neither '{}' exists, specify the location of hmm \
            profile with --path_func_annot option".format(fa_path, 'bank_hmm'))
        is_func_annot = True
    elif args.path_func_annot and args.no_proteins is False:
        FA_HMM = scan_hmm_bank(args.path_func_annot)
        is_func_annot = True
    else:
        is_func_annot = False

    if is_func_annot and not FA_HMM:
        print >> sys.stderr, "WARNING: No hmm profiles for functional annotation detected, \
        skip functional annotation step."


    if len(args.attc_model.split("/")) > 1: #contain path
        MODEL_attc = args.attc_model
    else:
        MODEL_attc = os.path.join(MODEL_DIR, args.attc_model)

    model_attc_name = MODEL_attc.split("/")[-1].split(".cm")[0]

    max_attc_size = args.max_attc_size
    min_attc_size = args.min_attc_size

    with open(MODEL_attc) as f:
        for i,line in enumerate(f):
            if "CLEN" in line:
                length_cm = int(line.split()[1])
                break


    if args.gembase:
        PROT_dir = os.path.join(in_dir, "..", "Proteins")
        PROT_file = os.path.join(PROT_dir, replicon_name + ".prt")
    else:
        PROT_file = os.path.join(out_dir, replicon_name + ".prt")

    ############### Default search ###############

    intI_file = os.path.join(out_dir, replicon_name + "_intI.res")
    phageI_file = os.path.join(out_dir, replicon_name + "_phage_int.res")
    attC_default_file = os.path.join(out_dir, replicon_name + "_attc_table.res")

    if args.no_proteins == False:
        if (os.path.isfile(intI_file) == 0 or
            os.path.isfile(phageI_file) == 0):

            find_integrase(replicon_path, replicon_name, out_dir)


    print "\n>>> Starting Default search ... :"
    if os.path.isfile(attC_default_file) == 0:
        find_attc(replicon_path, replicon_name, out_dir)

    print ">>> Default search done... : \n"
    integrons = find_integron(replicon_name,
                              attC_default_file,
                              intI_file,
                              phageI_file)

    ############### Search with local_max ###############
    if (args.eagle_eyes or args.local_max):

        print "\n>>>>>> Starting search with local_max...:"
        if os.path.isfile(os.path.join(out_dir, "integron_max.pickle")) == 0:


            integron_max = find_attc_max(integrons)
            integron_max.to_pickle(os.path.join(out_dir, "integron_max.pickle"))
            print ">>>>>> Search with local_max done... : \n"

        else:
            integron_max = pd.read_pickle(os.path.join(out_dir,
                                                       "integron_max.pickle"))
            print ">>>>>> Search with local_max was already done, continue... : \n"

        integrons = find_integron(replicon_name,
                                  integron_max,
                                  intI_file,
                                  phageI_file)


    ############### Add promoters and attI ###############

    outfile = replicon_name + ".integrons"

    if len(integrons):
        j = 1
        for i in integrons:
            if i.type() != "In0": # complete & CALIN
                if args.no_proteins == False:
                    i.add_proteins()

            if i.type() == "complete":
                i.add_promoter()
                i.add_attI()
                j += 1
            if i.type() == "In0":
                i.add_attI()
                i.add_promoter()

        ############### Functional annotation ###############

        if is_func_annot and len(FA_HMM) > 0:
            func_annot(replicon_name, out_dir, FA_HMM)

        j = 1
        for i in integrons:
            if i.type() == "complete":
                i.draw_integron(file=os.path.join(out_dir_ok,
                                                  replicon_name + "_" + str(j) + ".pdf"))
                j += 1

        ############### Writing out results ###############

        integrons_describe = pd.concat([i.describe() for i in integrons])
        dic_id = {i: "%02i" % (j + 1) for j, i in enumerate(integrons_describe.sort_values("pos_beg").ID_integron.unique())}
        integrons_describe.ID_integron = ["integron_" + dic_id[i] for i in integrons_describe.ID_integron]
        integrons_describe = integrons_describe[["ID_integron", "ID_replicon", "element",
                                                 "pos_beg", "pos_end", "strand", "evalue",
                                                 "type_elt", "annotation", "model",
                                                 "type", "default", "distance_2attC"]]
        integrons_describe['evalue'] = integrons_describe.evalue.astype(float)
        integrons_describe.index = range(len(integrons_describe))

        integrons_describe.sort_values(["ID_integron", "pos_beg", "evalue"], inplace=True)

        integrons_describe.to_csv(os.path.join(out_dir_ok, outfile), sep="\t", index=0, na_rep="NA")
        to_gbk(integrons_describe, SEQUENCE)
        if not SEQUENCE.description.endswith('.'):
            SEQUENCE.description += '.'
        SeqIO.write(SEQUENCE, os.path.join(out_dir_ok, replicon_name + ".gbk"), "genbank")
    else:
        out_f = open(os.path.join(out_dir_ok, outfile), "w")
        out_f.write("# No Integron found\n")
        out_f.close()
